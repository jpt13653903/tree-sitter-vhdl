import core

import std.env
import std.standard
import std.textio

import ieee.std_logic_1164
import ieee.numeric_std
import ieee.fixed_pkg
import ieee.float_pkg
import ieee.math_real
import ieee.math_complex
#-------------------------------------------------------------------------------

token_list = []

core.register(token_list)

std.env     .register(token_list)
std.standard.register(token_list)
std.textio  .register(token_list)

ieee.std_logic_1164.register(token_list)
ieee.numeric_std   .register(token_list)
ieee.fixed_pkg     .register(token_list)
ieee.float_pkg     .register(token_list)
ieee.math_real     .register(token_list)
ieee.math_complex  .register(token_list)
#-------------------------------------------------------------------------------

class Node:
    def __init__(self):
        self.type = []
        self.next = {}

token_tree = {}

for keyword, type in token_list:
    last = None
    root = token_tree
    for character in keyword.lower():
        if character not in root:
            root[character] = Node()
        if character == '_' and len(root[character].type) == 0:
            root[character].type = [ 'IDENTIFIER_EXPECTING_LETTER' ]
        last = root[character]
        root = last.next
    if last and type not in last.type:
        last.type.insert(0, type)
#-------------------------------------------------------------------------------

def sanitise(character):
    match character:
        case '\n': return '\\n'
        case '\r': return '\\r'
        case _: return character
#-------------------------------------------------------------------------------

def gen_case(file, root, indent = 0):
    if len(root) == 0:
        return

    file.write(indent*' ' + 'switch(lookahead){\n')
    indent += 2;
    for character, node in root.items():
        file.write(indent*' ' + f"case '{sanitise(character)}':\n")

        file.write((indent+2)*' ' + f'lookahead = advance(lexer);\n')
        if len(node.type) > 0:
            file.write((indent+2)*' ' + f'lexer->mark_end(lexer);\n')
            file.write((indent+2)*' ' + '{\n')
            file.write((indent+4)*' ' + 'static const TokenType type_array[] = { ')
            for type in node.type:
                file.write(f'{type}, ')
            file.write('0 };\n')
            file.write((indent+4)*' ' + f'type = type_array;\n')
            file.write((indent+2)*' ' + '}\n')
        else:
            file.write((indent+2)*' ' + f'type = 0;\n')
        gen_case(file, node.next, indent+2)
        file.write((indent+2)*' ' + f'break;\n')

    file.write(indent*' ' + 'default:\n')
    file.write((indent+2)*' ' + 'break;\n')

    indent -= 2
    file.write(indent*' ' + '}\n')
#-------------------------------------------------------------------------------

with open('../src/token_tree_match.inc', 'w', newline='\n', encoding='utf-8') as file:
    file.write('/*------------------------------------------------------------------------------\n')
    file.write('\n')
    file.write('Autogenerated by autogen/gen_token_trie.py\n')
    file.write('------------------------------------------------------------------------------*/\n')
    file.write('\n')
    gen_case(file, token_tree)
    file.write('//------------------------------------------------------------------------------\n')
#-------------------------------------------------------------------------------

